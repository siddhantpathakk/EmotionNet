{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "from src.model import EmotionNet\n",
    "from src.dataloader import get_MELD_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_speakers_to_int(videoSpeakers):\n",
    "    unique_speakers = {}\n",
    "    speaker_id = 1  # Start numbering speakers from 1\n",
    "\n",
    "    # Assign a unique integer to each unique speaker vector\n",
    "    for speaker_vector in videoSpeakers:\n",
    "\n",
    "        # Convert the vector to a tuple to use it as a key in the dictionary\n",
    "        speaker_tuple = tuple(speaker_vector)\n",
    "        if speaker_tuple not in unique_speakers:\n",
    "            unique_speakers[speaker_tuple] = speaker_id\n",
    "            speaker_id += 1\n",
    "\n",
    "    return unique_speakers, len(unique_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0: 'neutral', 1: 'surprise',\n",
    "              2: 'fear', 3: 'sadness',\n",
    "              4: 'joy', 5: 'disgust', 6: 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true',\n",
    "                        default=True, help='does not use CUDA')\n",
    "    parser.add_argument('--dir', type=str, default='./MELD_features/',\n",
    "                        help='dataset directory (for .pkl file)')\n",
    "    parser.add_argument('--n-classes', type=int, default=7,\n",
    "                        help='number of classes')\n",
    "    parser.add_argument('--val-split', type=float,\n",
    "                        default=0.1, help='validation split')\n",
    "    parser.add_argument('--num-workers', type=int,\n",
    "                        default=0, help='number of workers')\n",
    "\n",
    "    parser.add_argument('--loss-fn', type=str, default='masked_nll',\n",
    "                        help='loss function (masked_nll or unmaksed_weighted_nll or masked_mse)')\n",
    "    parser.add_argument('--optimizer', type=str, default='sgd',\n",
    "                        help='optimizer (adam or sgd or rmsprop)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        metavar='LR', help='learning rate')\n",
    "    parser.add_argument('--l2', type=float, default=3e-4,\n",
    "                        metavar='L2', help='L2 regularization weight')\n",
    "    parser.add_argument('--dropout', type=float, default=0.25,\n",
    "                        metavar='dropout', help='dropout rate')\n",
    "    parser.add_argument('--batch-size', type=int, default=20,\n",
    "                        metavar='BS', help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                        metavar='E', help='number of epochs')\n",
    "\n",
    "    parser.add_argument('--class-weight', action='store_true',\n",
    "                        default=True, help='use class weights (true or false)')\n",
    "    parser.add_argument('--mu', type=float, default=0,\n",
    "                        help='class weight (mu)')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        metavar='seed', help='seed')\n",
    "\n",
    "    parser.add_argument('--feature_type', type=str, default='multimodal',\n",
    "                        help='features (text or audio or multimodal)')\n",
    "    parser.add_argument('--attention', type=str, default='general',\n",
    "                        help='attention type (simple or general or general2 or concat or dot)')\n",
    "\n",
    "    parser.add_argument('--verbose', action='store_true',\n",
    "                        default=True, help='verbose (true or false)')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_opt()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_m = 900\n",
    "D_g = D_q = D_r = 150\n",
    "D_h = D_e = 100\n",
    "\n",
    "model = EmotionNet(D_m, D_q, D_g, D_r, D_e, D_h, n_classes=args.n_classes, dropout=args.dropout, attention=args.attention)\n",
    "\n",
    "# load the model ckpt\n",
    "model.load_state_dict(torch.load('model_v2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features_pkl_filepath = './MELD_features/MELD_features_raw.pkl'\n",
    "\n",
    "videoIDs, videoSpeakers, videoLabels, videoText, \\\n",
    "    videoAudio, videoSentence, trainVid, testVid, vids = pickle.load(\n",
    "        open(raw_features_pkl_filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader , valid_loader , test_loader = get_MELD_loaders(path=raw_features_pkl_filepath, n_classes=7, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on train set\n",
      "\n",
      "Video conversation #959\n",
      "Speakers in the video : 2\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t I didnt even realise how late it was, until I noticed the 5 oclock shadow on her head.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t Anyway, she didnt want to stay.\n",
      "[actual: sadness]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t I called a cab. she just left.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: neutral] Speaker 2 \t:\t I wrote you a letter.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: joy] Speaker 1 \t:\t Ohh! Thank you! I like mail.\n",
      "[actual: joy]\n",
      "\n",
      "[pred: neutral] Speaker 2 \t:\t Its just some things Ive been thinking about.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t Some things about us, and before we can even think about the two of us getting back together, I just need to know how you feel about this stuff.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: disgust] Speaker 1 \t:\t Okay.  Wow, its-its 5:30 in the morning.  So, Id better get cracking on this baby.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t Well, Ill be waiting for you, just come up when youre done.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: joy] Speaker 1 \t:\t Okay, Ill be up in,  18 pages. Front and back. Very exciting.\n",
      "[actual: joy]\n",
      "\n",
      "\n",
      "Accuracy : 60.000%\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluating on train set')\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "\n",
    "textf, acouf, qmask, umask, label = [\n",
    "    d.to('cuda') for d in data[:-1]] if args.cuda else data[:-1]\n",
    "\n",
    "log_prob, alpha_f, alpha_b = model(torch.cat((textf,acouf),dim=-1), qmask,umask) # seq_len, batch, n_classes\n",
    "lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
    "pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
    "pred = pred_.data.cpu().numpy()\n",
    "pred\n",
    "\n",
    "random_idx = data[-1][0]\n",
    "print(f'\\nVideo conversation #{random_idx}')\n",
    "\n",
    "speakers, unique_speakers = map_speakers_to_int(videoSpeakers[random_idx])\n",
    "\n",
    "print(f'Speakers in the video : {unique_speakers}\\n')\n",
    "count = total = 0\n",
    "for i, j, k, p in zip(videoSentence[random_idx], videoSpeakers[random_idx], videoLabels[random_idx], pred):\n",
    "    if p == k:\n",
    "        count += 1\n",
    "        \n",
    "    print(\n",
    "        f'[pred: {labels_map[p]}]',\n",
    "        f'Speaker {speakers[tuple(j)]}',\n",
    "        f'\\t:\\t {i}')\n",
    "    print(f'[actual: {labels_map[k]}]')\n",
    "    print()\n",
    "    total += 1\n",
    "\n",
    "print(f'\\nAccuracy : {count/total*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set\n",
      "\n",
      "Video conversation #94\n",
      "Speakers in the video : 2\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t My drinking?\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: fear] Speaker 2 \t:\t Oh, I mustve said that after you left.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: surprise] Speaker 1 \t:\t Said what? Exactly.\n",
      "[actual: fear]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t That you enjoyed the occasional drinking binge.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t Oh my God!! Ohh, that is it! Im leaving! You are just a horrible person!\n",
      "[actual: disgust]\n",
      "\n",
      "[pred: anger] Speaker 2 \t:\t Wait-wait-wait-wait-wait-wait-wait-wait!!\n",
      "[actual: fear]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t If youre gonna get all sensitive about it!\n",
      "[actual: anger]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t I dont want to lose you.\n",
      "[actual: sadness]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t What if I, create a position for you?\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t Ill make you an assistant buyer in this department.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: sadness] Speaker 1 \t:\t Say more things like that.\n",
      "[actual: joy]\n",
      "\n",
      "[pred: disgust] Speaker 2 \t:\t You can have your own office, and a raise! Effective tomorrow.\n",
      "[actual: joy]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t Id need an expense account.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: joy] Speaker 2 \t:\t Done!\n",
      "[actual: joy]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t And an assistant.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: joy] Speaker 2 \t:\t Sophie, get in here!\n",
      "[actual: joy]\n",
      "\n",
      "\n",
      "Accuracy : 37.500%\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluating on validation set')\n",
    "\n",
    "data = next(iter(valid_loader))\n",
    "\n",
    "textf, acouf, qmask, umask, label = [\n",
    "    d.to('cuda') for d in data[:-1]] if args.cuda else data[:-1]\n",
    "\n",
    "log_prob, alpha_f, alpha_b = model(\n",
    "    torch.cat((textf, acouf), dim=-1), qmask, umask)  # seq_len, batch, n_classes\n",
    "lp_ = log_prob.transpose(0, 1).contiguous(\n",
    ").view(-1, log_prob.size()[2])  # batch*seq_len, n_classes\n",
    "pred_ = torch.argmax(lp_, 1)  # batch*seq_len\n",
    "pred = pred_.data.cpu().numpy()\n",
    "pred\n",
    "\n",
    "random_idx = data[-1][0]\n",
    "print(f'\\nVideo conversation #{random_idx}')\n",
    "\n",
    "speakers, unique_speakers = map_speakers_to_int(videoSpeakers[random_idx])\n",
    "\n",
    "print(f'Speakers in the video : {unique_speakers}\\n')\n",
    "count = total = 0\n",
    "for i, j, k, p in zip(videoSentence[random_idx], videoSpeakers[random_idx], videoLabels[random_idx], pred):\n",
    "    if p == k:\n",
    "        count += 1\n",
    "    print(\n",
    "        f'[pred: {labels_map[p]}]',\n",
    "        f'Speaker {speakers[tuple(j)]}',\n",
    "        f'\\t:\\t {i}')\n",
    "    print(f'[actual: {labels_map[k]}]')\n",
    "    print()\n",
    "\n",
    "    total += 1\n",
    "\n",
    "print(f'\\nAccuracy : {count/total*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set\n",
      "[5 3]\n",
      "\n",
      "Video conversation #1360\n",
      "Speakers in the video : 2\n",
      "\n",
      "[pred: disgust] Speaker 1 \t:\t Ah, oh God. Oh, honey, oh that's OK.\n",
      "[actual: sadness]\n",
      "\n",
      "[pred: sadness] Speaker 2 \t:\t What. Oh no, you just rolled over the juice box.\n",
      "[actual: sadness]\n",
      "\n",
      "\n",
      "Accuracy : 50.000%\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluating on test set')\n",
    "\n",
    "data = next(iter(test_loader))\n",
    "textf, acouf, qmask, umask, label = [\n",
    "    d.to('cuda') for d in data[:-1]] if args.cuda else data[:-1]\n",
    "\n",
    "log_prob, alpha_f, alpha_b = model(\n",
    "    torch.cat((textf, acouf), dim=-1), qmask, umask)  # seq_len, batch, n_classes\n",
    "lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])  # batch*seq_len, n_classes\n",
    "pred_ = torch.argmax(lp_, 1)  # batch*seq_len\n",
    "pred = pred_.data.cpu().numpy()\n",
    "print(pred)\n",
    "\n",
    "random_idx = data[-1][0]\n",
    "print(f'\\nVideo conversation #{random_idx}')\n",
    "\n",
    "speakers, unique_speakers = map_speakers_to_int(videoSpeakers[random_idx])\n",
    "\n",
    "print(f'Speakers in the video : {unique_speakers}\\n')\n",
    "count = total = 0\n",
    "for i, j, k, p in zip(videoSentence[random_idx], videoSpeakers[random_idx], videoLabels[random_idx], pred):\n",
    "    if p == k:\n",
    "        count += 1\n",
    "    print(\n",
    "        f'[pred: {labels_map[p]}]',\n",
    "        f'Speaker {speakers[tuple(j)]}',\n",
    "        f'\\t:\\t {i}')\n",
    "    print(f'[actual: {labels_map[k]}]')\n",
    "    print()\n",
    "    total += 1\n",
    "\n",
    "print(f'\\nAccuracy : {count/total*100:.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

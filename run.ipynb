{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the .pkl raw data and understanding how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = 'audio/MELD_features/MELD_features_raw.pkl'\n",
    "videoIDs, videoSpeakers, videoLabels, videoText, videoAudio, videoSentence, trainVid, testVid, _ = pd.read_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoAudio[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dialogue #1007 with 4 speakers, 16 utterances.\n",
      "\n",
      "[#1] Person 1: Tell us what happened, Brown Bird Ross. (neutral)\n",
      "[#2] Person 3: Well, I lost. (sadness)\n",
      "[#3] Person 3: Some little girl loaned her uniform to her nineteen year old sister, who went down to the U.S.S. (sadness)\n",
      "[#4] Person 3: Nimitz, and sold over 2,000 boxes. (anger)\n",
      "[#5] Person 1: Hey! Howd the interview go? (joy)\n",
      "[#6] Person 4: Oh, I blew it. I wouldnt of even hired me. (sadness)\n",
      "[#7] Person 3: Oh, come here sweetie, listen, youre gonna go on like a thousand interviews before you get a job. (sadness)\n",
      "[#8] Person 3: Thats not how that was supposed to come out. (sadness)\n",
      "[#9] Person 2: This is the worst Christmas ever. (sadness)\n",
      "[#10] Person 1: Y'know what Rach, maybe you should just, y'know stay here at the coffee house. (neutral)\n",
      "[#11] Person 4: I cant! (anger)\n",
      "[#12] Person 4: Its too late! (anger)\n",
      "[#13] Person 4: Terry already hired that girl over there. (sadness)\n",
      "[#14] Person 4: Look at her, shes even got waitress experience. (sadness)\n",
      "[#15] Person 4: Last night she was teaching everybody how to make napkin....  swans. (sadness)\n",
      "[#16] Person 3: That word was swans. (neutral)\n"
     ]
    }
   ],
   "source": [
    "# label_map = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger':6}\n",
    "num_to_label_map = {0: 'neutral', 1: 'surprise', 2: 'fear', 3: 'sadness', 4: 'joy', 5: 'disgust', 6: 'anger'}\n",
    "\n",
    "random_idx = random.randint(0, len(videoSentence))\n",
    "\n",
    "list_of_speakers = set(''.join(str(e) for e in speaker) for speaker in videoSpeakers[random_idx])\n",
    "num_of_speakers = len(list_of_speakers)\n",
    "nums = [i+1 for i in range(num_of_speakers)]\n",
    "speaker_dict = dict(zip(list_of_speakers, nums))\n",
    "\n",
    "print(f\"Loading dialogue #{random_idx} with {num_of_speakers} speakers, {len(videoSentence[random_idx])} utterances.\\n\")\n",
    "\n",
    "i=1\n",
    "for sentence, speaker, label in zip(videoSentence[random_idx], videoSpeakers[random_idx], videoLabels[random_idx]):\n",
    "    speaker = ''.join(str(e) for e in speaker)\n",
    "\n",
    "    print(f'[#{i}] Person {speaker_dict[speaker]}: {sentence} ({num_to_label_map[label]})')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_cuda=True, dir='MELD_features/', lr=0.0001, l2=0.0003, dropout=0.5, batch_size=8, epochs=2, class_weight=False, seed=100, mu=0, verbose=True)\n",
      "Running on CPU\n",
      "EmoNet model.\n",
      "EmoNet(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (emo_rnn_b): EmotionRNN(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (cell): EmotionGRUCell(\n",
      "      (g_cell): GRUCell(600, 150)\n",
      "      (p_cell): GRUCell(450, 150)\n",
      "      (pl_cell): GRUCell(450, 150)\n",
      "      (r_cell): GRUCell(450, 150)\n",
      "      (rl_cell): GRUCell(450, 150)\n",
      "      (e_cell): GRUCell(600, 150)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (attention): SimpleAttention(\n",
      "        (scalar): Linear(in_features=150, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (emo_rnn_f): EmotionRNN(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (cell): EmotionGRUCell(\n",
      "      (g_cell): GRUCell(600, 150)\n",
      "      (p_cell): GRUCell(450, 150)\n",
      "      (pl_cell): GRUCell(450, 150)\n",
      "      (r_cell): GRUCell(450, 150)\n",
      "      (rl_cell): GRUCell(450, 150)\n",
      "      (e_cell): GRUCell(600, 150)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (attention): SimpleAttention(\n",
      "        (scalar): Linear(in_features=150, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (smax_fc): Linear(in_features=100, out_features=7, bias=True)\n",
      ")\n",
      "Loss function:\tMaskedNLLLoss\n",
      "Optimizer:\tAdam\n",
      "\n",
      "Training started..\n",
      "Epoch:[1]/[2]\ttrain_loss:1.670\ttrain_acc:43.330\ttrain_fscore:30.330\tval_loss:1.528\tval_acc:47.450\tval_fscore:30.540\ttest_loss:1.525\ttest_acc:48.120\ttest_fscore:31.270\ttime:29.05 sec\n",
      "Epoch:[2]/[2]\ttrain_loss:1.612\ttrain_acc:45.360\ttrain_fscore:30.380\tval_loss:1.507\tval_acc:47.450\tval_fscore:30.540\ttest_loss:1.505\ttest_acc:48.120\ttest_fscore:31.270\ttime:24.92 sec\n",
      "\n",
      "Test performance..\n",
      "Fscore: 31.27 accuracy: 48.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4812    1.0000    0.6498    1256.0\n",
      "           1     0.0000    0.0000    0.0000     281.0\n",
      "           2     0.0000    0.0000    0.0000      50.0\n",
      "           3     0.0000    0.0000    0.0000     208.0\n",
      "           4     0.0000    0.0000    0.0000     402.0\n",
      "           5     0.0000    0.0000    0.0000      68.0\n",
      "           6     0.0000    0.0000    0.0000     345.0\n",
      "\n",
      "    accuracy                         0.4812    2610.0\n",
      "   macro avg     0.0687    0.1429    0.0928    2610.0\n",
      "weighted avg     0.2316    0.4812    0.3127    2610.0\n",
      "\n",
      "[[1256.    0.    0.    0.    0.    0.    0.]\n",
      " [ 281.    0.    0.    0.    0.    0.    0.]\n",
      " [  50.    0.    0.    0.    0.    0.    0.]\n",
      " [ 208.    0.    0.    0.    0.    0.    0.]\n",
      " [ 402.    0.    0.    0.    0.    0.    0.]\n",
      " [  68.    0.    0.    0.    0.    0.    0.]\n",
      " [ 345.    0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "Model saved to models/EmoNet_31.pt.\n",
      "\n",
      "Metrics saved to MELD_features/logs/metrics.csv.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.encoder import *\n",
    "from audio.dataloader import *\n",
    "from audio.model import *\n",
    "from audio.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "global D_s\n",
    "D_m = 300\n",
    "D_g = D_q = D_r = D_e = 150\n",
    "D_h = 100\n",
    "n_classes = 7\n",
    "\n",
    "model = EmoNet(D_m, D_q, D_g, D_r, D_e, D_h, n_classes=n_classes)\n",
    "\n",
    "saved_model_ckpt = \"audio/MELD_features/models/EmoNet_31.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(saved_model_ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, _, _ = get_MELD_loaders(pkl_path,\n",
    "                                    7,\n",
    "                                    valid=0.2,\n",
    "                                    batch_size=32,\n",
    "                                    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: neutral\n",
      "Actual: fear\n",
      "\n",
      "Predicted: neutral\n",
      "Actual: fear\n",
      "\n",
      "Predicted: joy\n",
      "Actual: neutral\n",
      "\n",
      "Predicted: neutral\n",
      "Actual: fear\n",
      "\n",
      "Predicted: joy\n",
      "Actual: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    textf, acouf, qmask, umask, label = data[:-1]\n",
    "    \n",
    "labels_ = label.view(-1).data.cpu().numpy()\n",
    "\n",
    "log_prob, alpha_f, alpha_b = model(acouf, qmask, umask)\n",
    "\n",
    "lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
    "pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
    "preds = pred_.data.cpu().numpy()\n",
    "\n",
    "for i in preds[:5]:\n",
    "    print(f\"Predicted: {num_to_label_map[i]}\\nActual: {num_to_label_map[labels_[i].item()]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenSMILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensmile\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opensmile_feature(audio_file_path, feature_type):\n",
    "    if feature_type == \"emobase\":\n",
    "        smile = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.emobase,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    elif feature_type == \"ComParE\":\n",
    "        smile = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    else:\n",
    "        smile = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    return np.array(smile.process_file(audio_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested in https://aclanthology.org/N18-1193.pdf, we use openSMILE with the ComParE configuration to get 6373 features for each utterance video. Z- standardization is performed for voice normaliza- tion and dimension of the audio vector is reduced to 100 using a fully-connected neural layer. This provides the final audio feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_vector(path):\n",
    "    \n",
    "    # Extract ComParE features to (1, 6373)\n",
    "    a = opensmile_feature(path, 'ComParE')\n",
    "\n",
    "    # Z-score standardization\n",
    "    mean = np.mean(a)\n",
    "    std_dev = np.std(a)\n",
    "    standardized_a= (a - mean) / std_dev\n",
    "\n",
    "    # Dimensionality reduction to (300,)\n",
    "    fc_layer = nn.Linear(6373, 300)\n",
    "    a_ = torch.tensor(standardized_a).float()\n",
    "    output_tensor = fc_layer(a_).detach().numpy().reshape(-1,)\n",
    "    \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_emb = get_audio_vector('audio/tmp/examples/utt0.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_allclose\n",
    "assert_allclose(actual=output_tensor, desired=videoAudio[1][0], atol=2, err_msg='The output tensor is different.', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our audio features are nearly same to the ones provided as part of open-source code. The difference in exact values arises due to the use of a fully connected layer to reduce the dimension of the audio vector to 300, since each initialisation of weights and biases is done randomly, thus unable to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/tmp/examples/dia1_utt8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "eg = make_embs_for_dialogue('audio/tmp/examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoAudio[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "from src.model import EmotionNet\n",
    "from src.dataloader import get_MELD_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_speakers_to_int(videoSpeakers):\n",
    "    unique_speakers = {}\n",
    "    speaker_id = 1  # Start numbering speakers from 1\n",
    "\n",
    "    # Assign a unique integer to each unique speaker vector\n",
    "    for speaker_vector in videoSpeakers:\n",
    "\n",
    "        # Convert the vector to a tuple to use it as a key in the dictionary\n",
    "        speaker_tuple = tuple(speaker_vector)\n",
    "        if speaker_tuple not in unique_speakers:\n",
    "            unique_speakers[speaker_tuple] = speaker_id\n",
    "            speaker_id += 1\n",
    "\n",
    "    return unique_speakers, len(unique_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0: 'neutral', 1: 'surprise',\n",
    "              2: 'fear', 3: 'sadness',\n",
    "              4: 'joy', 5: 'disgust', 6: 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true',\n",
    "                        default=True, help='does not use CUDA')\n",
    "    parser.add_argument('--dir', type=str, default='./MELD_features/',\n",
    "                        help='dataset directory (for .pkl file)')\n",
    "    parser.add_argument('--n-classes', type=int, default=7,\n",
    "                        help='number of classes')\n",
    "    parser.add_argument('--val-split', type=float,\n",
    "                        default=0.1, help='validation split')\n",
    "    parser.add_argument('--num-workers', type=int,\n",
    "                        default=0, help='number of workers')\n",
    "\n",
    "    parser.add_argument('--loss-fn', type=str, default='masked_nll',\n",
    "                        help='loss function (masked_nll or unmaksed_weighted_nll or masked_mse)')\n",
    "    parser.add_argument('--optimizer', type=str, default='sgd',\n",
    "                        help='optimizer (adam or sgd or rmsprop)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        metavar='LR', help='learning rate')\n",
    "    parser.add_argument('--l2', type=float, default=3e-4,\n",
    "                        metavar='L2', help='L2 regularization weight')\n",
    "    parser.add_argument('--dropout', type=float, default=0.25,\n",
    "                        metavar='dropout', help='dropout rate')\n",
    "    parser.add_argument('--batch-size', type=int, default=20,\n",
    "                        metavar='BS', help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                        metavar='E', help='number of epochs')\n",
    "\n",
    "    parser.add_argument('--class-weight', action='store_true',\n",
    "                        default=True, help='use class weights (true or false)')\n",
    "    parser.add_argument('--mu', type=float, default=0,\n",
    "                        help='class weight (mu)')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        metavar='seed', help='seed')\n",
    "\n",
    "    parser.add_argument('--feature_type', type=str, default='multimodal',\n",
    "                        help='features (text or audio or multimodal)')\n",
    "    parser.add_argument('--attention', type=str, default='general',\n",
    "                        help='attention type (simple or general or general2 or concat or dot)')\n",
    "\n",
    "    parser.add_argument('--verbose', action='store_true',\n",
    "                        default=True, help='verbose (true or false)')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_opt()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_m = 900\n",
    "D_g = D_q = D_r = 150\n",
    "D_h = D_e = 100\n",
    "\n",
    "model = EmotionNet(D_m, D_q, D_g, D_r, D_e, D_h, n_classes=args.n_classes, dropout=args.dropout, attention=args.attention)\n",
    "\n",
    "# load the model ckpt\n",
    "model.load_state_dict(torch.load('model_v2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features_pkl_filepath = './MELD_features/MELD_features_raw.pkl'\n",
    "\n",
    "videoIDs, videoSpeakers, videoLabels, videoText, \\\n",
    "    videoAudio, videoSentence, trainVid, testVid, vids = pickle.load(\n",
    "        open(raw_features_pkl_filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader , valid_loader , test_loader = get_MELD_loaders(path=raw_features_pkl_filepath, n_classes=7, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on train set\n",
      "\n",
      "Video conversation #221\n",
      "Speakers in the video : 2\n",
      "\n",
      "[pred: surprise] Speaker 1 \t:\t Your parents?\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: neutral] Speaker 2 \t:\t Yeah, theyre out of town.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: joy] Speaker 1 \t:\t Ohh.\n",
      "[actual: surprise]\n",
      "\n",
      "[pred: neutral] Speaker 2 \t:\t Yeah-yeah, its this\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t Yeah that works.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: fear] Speaker 2 \t:\t They-they-they can smell fear.\n",
      "[actual: fear]\n",
      "\n",
      "\n",
      "Accuracy : 66.667%\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluating on train set')\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "\n",
    "textf, acouf, qmask, umask, label = [\n",
    "    d.to('cuda') for d in data[:-1]] if args.cuda else data[:-1]\n",
    "\n",
    "log_prob, alpha_f, alpha_b = model(torch.cat((textf,acouf),dim=-1), qmask,umask) # seq_len, batch, n_classes\n",
    "lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
    "pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
    "pred = pred_.data.cpu().numpy()\n",
    "pred\n",
    "\n",
    "random_idx = data[-1][0]\n",
    "print(f'\\nVideo conversation #{random_idx}')\n",
    "\n",
    "speakers, unique_speakers = map_speakers_to_int(videoSpeakers[random_idx])\n",
    "\n",
    "print(f'Speakers in the video : {unique_speakers}\\n')\n",
    "count = total = 0\n",
    "for i, j, k, p in zip(videoSentence[random_idx], videoSpeakers[random_idx], videoLabels[random_idx], pred):\n",
    "    if p == k:\n",
    "        count += 1\n",
    "        \n",
    "    print(\n",
    "        f'[pred: {labels_map[p]}]',\n",
    "        f'Speaker {speakers[tuple(j)]}',\n",
    "        f'\\t:\\t {i}')\n",
    "    print(f'[actual: {labels_map[k]}]')\n",
    "    print()\n",
    "    total += 1\n",
    "\n",
    "print(f'\\nAccuracy : {count/total*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set\n",
      "[0 0 0 6 6 6 6 6 6 0 6]\n",
      "\n",
      "Video conversation #1155\n",
      "Speakers in the video : 2\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t Okay.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: neutral] Speaker 2 \t:\t Ross, didn't you say that there was an elevator in here?\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t Uhh, yes I did but there isn't. Okay, here we go.\n",
      "[actual: sadness]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t Okay, go left. Left! Left!\n",
      "[actual: surprise]\n",
      "\n",
      "[pred: anger] Speaker 2 \t:\t Okay, y'know what? There is no more left, left!\n",
      "[actual: anger]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t Oh okay, lift it straight up over your head!\n",
      "[actual: anger]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t Straight up over your head!\n",
      "[actual: anger]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t You can do it!\n",
      "[actual: joy]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t You can do it!\n",
      "[actual: joy]\n",
      "\n",
      "[pred: neutral] Speaker 1 \t:\t Okay.\n",
      "[actual: neutral]\n",
      "\n",
      "[pred: anger] Speaker 1 \t:\t You got it?\n",
      "[actual: neutral]\n",
      "\n",
      "\n",
      "Accuracy : 54.545%\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluating on test set')\n",
    "\n",
    "data = next(iter(test_loader))\n",
    "textf, acouf, qmask, umask, label = [\n",
    "    d.to('cuda') for d in data[:-1]] if args.cuda else data[:-1]\n",
    "\n",
    "log_prob, alpha_f, alpha_b = model(\n",
    "    torch.cat((textf, acouf), dim=-1), qmask, umask)  # seq_len, batch, n_classes\n",
    "lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])  # batch*seq_len, n_classes\n",
    "pred_ = torch.argmax(lp_, 1)  # batch*seq_len\n",
    "pred = pred_.data.cpu().numpy()\n",
    "print(pred)\n",
    "\n",
    "random_idx = data[-1][0]\n",
    "print(f'\\nVideo conversation #{random_idx}')\n",
    "\n",
    "speakers, unique_speakers = map_speakers_to_int(videoSpeakers[random_idx])\n",
    "\n",
    "print(f'Speakers in the video : {unique_speakers}\\n')\n",
    "count = total = 0\n",
    "for i, j, k, p in zip(videoSentence[random_idx], videoSpeakers[random_idx], videoLabels[random_idx], pred):\n",
    "    if p == k:\n",
    "        count += 1\n",
    "    print(\n",
    "        f'[pred: {labels_map[p]}]',\n",
    "        f'Speaker {speakers[tuple(j)]}',\n",
    "        f'\\t:\\t {i}')\n",
    "    print(f'[actual: {labels_map[k]}]')\n",
    "    print()\n",
    "    total += 1\n",
    "\n",
    "print(f'\\nAccuracy : {count/total*100:.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
